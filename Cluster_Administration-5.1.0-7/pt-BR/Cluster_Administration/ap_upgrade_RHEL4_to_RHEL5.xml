<?xml version='1.0'?>
<!DOCTYPE appendix PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [
<!ENTITY % RH_ENTITIES SYSTEM "Common_Content/Entities.ent">
%RH_ENTITIES;
<!ENTITY % RH_TRANS_ENTITIES SYSTEM "Common_Content/Translatable-Entities.ent">
%RH_TRANS_ENTITIES;
]>

<appendix id="Cluster_Administration-Upgrading_A_RH_Cluster_from_RHEL_4_to_RHEL_5">
	<title>Upgrading A &RH; Cluster from RHEL 4 to RHEL 5</title>
	<indexterm>
		<primary>upgrading, RHEL 4 to RHEL 5</primary>
	</indexterm>
	<para>
		This appendix provides a procedure for upgrading a &RH; cluster from RHEL 4 to RHEL 5. The procedure includes changes required for &RHGFS; and CLVM, also. For more information about &RHGFS;, refer to <citetitle>Global File System: Configuration and Administration</citetitle>. For more information about LVM for clusters, refer to <citetitle>LVM Administrator&#39;s Guide: Configuration and Administration</citetitle>.
	</para>
	<para>
		Upgrading a &RH; Cluster from RHEL 4 to RHEL 5 consists of stopping the cluster, converting the configuration from a GULM cluster to a CMAN cluster (only for clusters configured with the GULM cluster manager/lock manager), adding node IDs, and updating RHEL and cluster software. To upgrade a &RH; Cluster from RHEL 4 to RHEL 5, follow these steps:
	</para>
	<orderedlist>
		<listitem>
			<para>
				Stop client access to cluster high-availability services.
			</para>
		</listitem>
		<listitem>
			<para>
				At each cluster node, stop the cluster software as follows:
			</para>
			<orderedlist>
				<listitem>
					<para>
						Stop all high-availability services.
					</para>
				</listitem>
				<listitem>
					<para>
						Run <command>service rgmanager stop</command>.
					</para>
				</listitem>
				<listitem>
					<para>
						Run <command>service gfs stop</command>.
					</para>
				</listitem>
				<listitem>
					<para>
						Run <command>service clvmd stop</command>.
					</para>
					<note>
						<title>Note</title>
						<para>
							If <command>clvmd</command> is already stopped, an error message is displayed:
						</para>
<screen>
# <userinput>service clvmd stop</userinput>
Stopping clvm:                                             [FAILED]
</screen>
						<para>
							The error message is the expected result when running <command>service clvmd stop</command> after <command>clvmd</command> has stopped.
						</para>
					</note>
				</listitem>
				<listitem>
					<para>
						Depending on the type of cluster manager (either CMAN or GULM), run the following command or commands:
					</para>
					<itemizedlist>
						<listitem>
							<para>
								CMAN &mdash; Run <command>service fenced stop; service cman stop</command>.
							</para>
						</listitem>
						<listitem>
							<para>
								GULM &mdash; Run <command>service lock_gulmd stop</command>.
							</para>
						</listitem>
					</itemizedlist>
				</listitem>
				<listitem>
					<para>
						Run <command>service ccsd stop</command>.
					</para>
				</listitem>
			</orderedlist>
		</listitem>
		<listitem>
			<para>
				Disable cluster software from starting during reboot. At each node, run <command>/sbin/chkconfig</command> as follows:
			</para>
<screen>
# <userinput>chkconfig --level 2345 rgmanager off</userinput>
# <userinput>chkconfig --level 2345 gfs off</userinput>
# <userinput>chkconfig --level 2345 clvmd off</userinput>
# <userinput>chkconfig --level 2345 fenced off</userinput>
# <userinput>chkconfig --level 2345 cman off</userinput>
# <userinput>chkconfig --level 2345 ccsd off</userinput></screen>
		</listitem>
		<listitem>
			<para>
				Edit the cluster configuration file as follows:
			</para>
			<orderedlist>
				<listitem>
					<para>
						At a cluster node, open <filename>/etc/cluster/cluster.conf</filename> with a text editor.
					</para>
				</listitem>
				<listitem>
					<para>
						If your cluster is configured with GULM as the cluster manager, remove the GULM XML elements &mdash; <command>&lt;gulm&gt;</command> and <command> &lt;/gulm&gt;</command> &mdash; and their content from <filename>/etc/cluster/cluster.conf</filename>. GULM is not supported in &RHCS; for RHEL 5. <xref linkend="Cluster_Administration-Upgrading_A_RH_Cluster_from_RHEL_4_to_RHEL_5-GULM_XML_Elements_and_Content" /> shows an example of GULM XML elements and content.
					</para>
				</listitem>
				<listitem>
					<para>
						At the <command>&lt;clusternode&gt;</command> element for each node in the configuration file, insert <command>nodeid="<replaceable>number</replaceable>"</command> after <command>name="<replaceable>name</replaceable>"</command>. Use a <replaceable>number</replaceable> value unique to that node. Inserting it there follows the format convention of the <command>&lt;clusternode&gt;</command> element in a RHEL 5 cluster configuration file.
					</para>
					<note>
						<title>Note</title>
						<para>
							The <command>nodeid</command> parameter is required in &RHCS; for RHEL 5. The parameter is optional in &RHCS; for RHEL 4. If your configuration file already contains <command>nodeid</command> parameters, skip this step.
						</para>
					</note>
				</listitem>
				<listitem>
					<para>
						When you have completed editing <filename>/etc/cluster/cluster.conf</filename>, save the file and copy it to the other nodes in the cluster (for example, using the <command>scp</command> command).
					</para>
				</listitem>
			</orderedlist>
		</listitem>
		<listitem>
			<para>
				If your cluster is a GULM cluster and uses &RHGFS;, change the superblock of each GFS file system to use the DLM locking protocol. Use the <command>gfs_tool</command> command with the <option>sb</option> and <option>proto</option> options, specifying <option>lock_dlm</option> for the DLM locking protocol:
			</para>
			<para>
				<command>gfs_tool sb <replaceable>device</replaceable> proto lock_dlm</command>
			</para>
			<para>
				For example:
			</para>
<screen>
# <userinput>gfs_tool sb /dev/my_vg/gfs1 proto lock_dlm</userinput>
You shouldn&#39;t change any of these values if the filesystem is mounted.

Are you sure? [y/n] <userinput>y</userinput>

current lock protocol name = "lock_gulm"
new lock protocol name = "lock_dlm"
Done
</screen>
		</listitem>
		<listitem>
			<para>
				Update the software in the cluster nodes to RHEL 5 and &RHCS; for RHEL 5. You can acquire and update software through &RHN; channels for RHEL 5 and &RHCS; for RHEL 5.
			</para>
		</listitem>
		<listitem>
			<para>
				Run <command>lvmconf --enable-cluster</command>.
			</para>
		</listitem>
		<listitem>
			<para>
				Enable cluster software to start upon reboot. At each node run <command>/sbin/chkconfig</command> as follows:
			</para>
<screen>
# <userinput>chkconfig --level 2345 rgmanager on</userinput>
# <userinput>chkconfig --level 2345 gfs on</userinput>
# <userinput>chkconfig --level 2345 clvmd on</userinput>
# <userinput>chkconfig --level 2345 cman on</userinput></screen>
		</listitem>
		<listitem>
			<para>
				Reboot the nodes. The RHEL 5 cluster software should start while the nodes reboot. Upon verification that the &RH; cluster is running, the upgrade is complete.
			</para>
		</listitem>
	</orderedlist>
	<example id="Cluster_Administration-Upgrading_A_RH_Cluster_from_RHEL_4_to_RHEL_5-GULM_XML_Elements_and_Content"><title>GULM XML Elements and Content</title>
<screen>
&lt;gulm&gt;
  &lt;lockserver name="gulmserver1"/&gt;
  &lt;lockserver name="gulmserver2"/&gt;
  &lt;lockserver name="gulmserver3"/&gt;
&lt;/gulm&gt;
</screen>
	</example>
</appendix>

