<?xml version='1.0'?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN" "http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd" [
<!ENTITY % RH_ENTITIES SYSTEM "./Common_Config/rh-entities.ent">
%RH_ENTITIES;
]>

<chapter id="ch-hardware">
	<title>Hardware Installation and Operating System Configuration</title>
	<indexterm>
		
		<primary>hardware installation</primary>
		<secondary>operating system configuration</secondary>
	</indexterm>
	<indexterm>
		
		<primary>operating system configuration</primary>
		<secondary>hardware installation</secondary>
	</indexterm>
	<para>
		 To set up the hardware configuration and install &PROD;, follow these steps: 
	</para>
	<itemizedlist>
		<listitem>
			<para>
				 Choose a cluster hardware configuration that meets the needs of applications and users; refer to <xref linkend="s1-hardware-choosing" />. 
			</para>
		</listitem>
		<listitem>
			<para>
				 Set up and connect the members and the optional console switch and network switch or hub; refer to <xref linkend="s1-hardware-cluster" />. 
			</para>
		</listitem>
		<listitem>
			<para>
				 Install and configure &PROD; on the cluster members; refer to <xref linkend="s1-hardware-linux" />. 
			</para>
		</listitem>
		<listitem>
			<para>
				 Set up the remaining cluster hardware components and connect them to the members; refer to <xref linkend="s1-hardware-connect" />. 
			</para>
		</listitem>
	</itemizedlist>
	<para>
		 After setting up the hardware configuration and installing &PROD;, install the cluster software. 
	</para>
	<section id="s1-hardware-choosing">
		<title>Choosing a Hardware Configuration</title>
		<indexterm>
			
			<primary>hardware configuration</primary>
			<secondary>choosing a configuration</secondary>
		</indexterm>
		<para>
			 With &RH; Cluster, you can use commodity hardware to set up a cluster that meets your needs for performance, availability, and data integrity. Cluster hardware ranges from low-cost minimum configurations that include only the components required for cluster operation, to high-end configurations that include redundant Ethernet channels, hardware RAID, and power switches. 
		</para>
		<para>
			 Regardless of configuration, the use of high-quality hardware in a cluster is recommended, as hardware malfunction is a primary cause of system down time. 
		</para>
		<para>
			 When choosing a cluster hardware configuration, consider the following: 
		</para>
		<variablelist>
			<varlistentry><term>Performance requirements of applications and users</term><listitem>
				<para>
					<indexterm>
						
						<primary>hardware configuration</primary>
						<secondary>performance considerations</secondary>
					</indexterm>
					 Choose a hardware configuration that provides adequate memory, CPU, and I/O resources. Make sure that the configuration can handle future increases in workload, too. 
				</para>
			</listitem>
			</varlistentry><varlistentry><term>Cost restrictions</term><listitem>
				<para>
					<indexterm>
						
						<primary>hardware configuration</primary>
						<secondary>cost restrictions</secondary>
					</indexterm>
					 The hardware configuration should meet budget requirements. For example, systems with multiple I/O ports usually cost more than low-end systems with fewer expansion capabilities. 
				</para>
			</listitem>
			</varlistentry><varlistentry><term>Availability requirements</term><listitem>
				<para>
					<indexterm>
						
						<primary>hardware configuration</primary>
						<secondary>availability considerations</secondary>
					</indexterm>
					 In a mission-critical production environment, a cluster hardware configuration must protect against all single points of failure, including: disk, storage interconnect, Ethernet channel, and power failure. Environments that can tolerate an interruption in availability (such as development environments) may not require as much protection. 
				</para>
			</listitem>
			</varlistentry><varlistentry><term>Data integrity under all failure conditions requirement</term><listitem>
				<para>
					<indexterm>
						
						<primary>hardware configuration</primary>
						<secondary>data integrity under all failure conditions</secondary>
					</indexterm>
					 Using appropriate fence devices in a cluster configuration ensures that data is protected under a variety of failure conditions. 
				</para>
			</listitem>
			</varlistentry>
		</variablelist>
		<section id="s2-hardware-minimumreq">
			<title>Minimum Hardware Requirements</title>
			<para>
				 A <firstterm>minimum hardware configuration</firstterm> includes only the hardware components that are required for cluster operation. The minimum hardware for &RH; Cluster consists of the following components: 
			</para>
			<indexterm>
				
				<primary>hardware configuration</primary>
				<secondary>minimum</secondary>
			</indexterm>
			<indexterm>
				
				<primary>minimum hardware configuration</primary>
			</indexterm>
			<itemizedlist>
				<listitem>
					<para>
						 At least two servers to run cluster services 
					</para>
				</listitem>
				<listitem>
					<para>
						 Ethernet connection for sending heartbeat pings and for client network access 
					</para>
				</listitem>
				<listitem>
					<para>
						 Network switch or hub to connect cluster nodes and resources 
					</para>
				</listitem>
				<listitem>
					<para>
						 A fence device 
					</para>
				</listitem>
			</itemizedlist>
			<indexterm>
				
				<primary>examples</primary>
				<secondary>minimum cluster configuration</secondary>
			</indexterm>
			<indexterm>
				
				<primary>minimum cluster configuration example</primary>
			</indexterm>
			<indexterm>
				
				<primary>cluster configuration</primary>
				<secondary>minimum</secondary>
				<tertiary>example</tertiary>
			</indexterm>
			<para>
				 The hardware components described in <xref linkend="tb-hardware-minicluster" /> can be used to set up a minimum cluster configuration. This configuration does not ensure data integrity under all failure conditions, because it does not include power switches. Note that this is a sample configuration; it is possible to set up a minimum configuration using other hardware. 
			</para>
			<warning><title>Warning</title>
			<para>
				 The minimum cluster configuration is not a supported solution and <emphasis>should not be used</emphasis> in a production environment, as it does not ensure data integrity under all failure conditions. 
			</para>
			</warning><indexterm>
				
				<primary>tables</primary>
				<secondary>minimum cluster configuration components</secondary>
			</indexterm>
			<table id="tb-hardware-minicluster">
				<title>Example of Minimum Cluster Configuration</title>
				<tgroup cols="2">
					<colspec colname="Hardware" colnum="1" colwidth="3*"></colspec><colspec colname="Description" colnum="2" colwidth="6*"></colspec><thead>
						<row>
							<entry>
								 Hardware
							</entry>
							<entry>
								 Description
							</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>
								 At least two server systems
							</entry>
							<entry>
								 Each system becomes a node exclusively for use in the cluster; system hardware requirements are similar to that of &PROD; &PRODVER;.
							</entry>
						</row>
						<row>
							<entry>
								 One network interface card (NIC) for each node
							</entry>
							<entry>
								 One network interface connects to a hub or switch for cluster connectivity.
							</entry>
						</row>
						<row>
							<entry>
								 Network cables with RJ45 connectors
							</entry>
							<entry>
								 Network cables connect to the network interface on each node for client access and heartbeat packets.
							</entry>
						</row>
						<row>
							<entry>
								 RAID storage enclosure
							</entry>
							<entry>
								 The RAID storage enclosure contains one controller with at least two host ports.
							</entry>
						</row>
						<row>
							<entry>
								 Two HD68 SCSI cables
							</entry>
							<entry>
								 Each cable connects one host bus adapter to one port on the RAID controller, creating two single-initiator SCSI buses.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
			<para>
				 The minimum hardware configuration is a cost-effective cluster configuration for development purposes; however, it contains components that can cause service outages if they fail. For example, if the RAID controller fails, then all cluster services become unavailable. 
			</para>
			<para>
				 To improve availability, protect against component failure, and ensure data integrity under all failure conditions, more hardware is required. Refer to <xref linkend="tb-hardware-protect" />. 
			</para>
			<indexterm>
				
				<primary>tables</primary>
				<secondary>availability and data integrity</secondary>
			</indexterm>
			<indexterm>
				
				<primary>availability and data integrity table</primary>
			</indexterm>
			<table id="tb-hardware-protect">
				<title>Improving Availability and Data Integrity</title>
				<tgroup cols="2">
					<colspec colname="ProtectAgainst" colnum="1" colwidth="5*"></colspec><colspec colname="Solution" colnum="2" colwidth="7*"></colspec><thead>
						<row>
							<entry>
								 Problem
							</entry>
							<entry>
								 Solution
							</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>
								 Disk failure
							</entry>
							<entry>
								 Hardware RAID to replicate data across multiple disks
							</entry>
						</row>
						<row>
							<entry>
								 RAID controller failure
							</entry>
							<entry>
								 Dual RAID controllers to provide redundant access to disk data
							</entry>
						</row>
						<row>
							<entry>
								 Network interface failure
							</entry>
							<entry>
								 Ethernet channel bonding and failover
							</entry>
						</row>
						<row>
							<entry>
								 Power source failure
							</entry>
							<entry>
								 Redundant uninterruptible power supply (UPS) systems
							</entry>
						</row>
						<row>
							<entry>
								 Machine failure
							</entry>
							<entry>
								 Power switches
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
			<para>
				<xref linkend="fig-hardware-hardware1" /> illustrates a hardware configuration with improved availability. The configuration uses a fence device (in this case, a network-attached power switch) and the nodes are configured for &RHGFS; storage attached to a Fibre Channel SAN switch. 
			</para>
			<figure id="fig-hardware-hardware1">
				<title>Hardware Configuration for Improved availability</title>
				<mediaobject>
					<imageobject>
						<imagedata fileref="images/hardware1.png" />
					</imageobject>
					<textobject><para>
						 A hardware configuration for improved availability. 
					</para>
					</textobject>
				</mediaobject>
			</figure>
			<para>
				 A hardware configuration that ensures data integrity under failure conditions should include the following components: 
			</para>
			<itemizedlist>
				<listitem>
					<para>
						 At least two servers to run cluster services 
					</para>
				</listitem>
				<listitem>
					<para>
						 Switched Ethernet connection between each node for heartbeat pings and for client network access 
					</para>
				</listitem>
				<listitem>
					<para>
						 Dual-controller RAID array or redundant access to SAN or other storage. 
					</para>
				</listitem>
				<listitem>
					<para>
						 Network power switches to enable each node to power-cycle the other nodes during the failover process 
					</para>
				</listitem>
				<listitem>
					<para>
						 Ethernet interfaces configured to use channel bonding 
					</para>
				</listitem>
				<listitem>
					<para>
						 At least two UPS systems for a highly-available source of power 
					</para>
				</listitem>
			</itemizedlist>
			<indexterm>
				
				<primary>examples</primary>
				<secondary>no single point of failure configuration</secondary>
			</indexterm>
			<indexterm>
				
				<primary>no single point of failure configuration</primary>
			</indexterm>
			<para>
				 The components described in <xref linkend="tb-hardware-nospof" /> can be used to set up a no single point of failure cluster configuration that includes two single-initiator SCSI buses and power switches to ensure data integrity under all failure conditions. Note that this is a sample configuration; it is possible to set up a no single point of failure configuration using other hardware. 
			</para>
			<indexterm>
				
				<primary>tables</primary>
				<secondary>no single point of failure configuration</secondary>
			</indexterm>
			<table id="tb-hardware-nospof">
				<title>Example of a No Single Point of Failure Configuration</title>
				<tgroup cols="2">
					<colspec colname="Hardware" colnum="1" colwidth="3*"></colspec><colspec colname="Description" colnum="2" colwidth="6*"></colspec><thead>
						<row>
							<entry>
								 Hardware
							</entry>
							<entry>
								 Description
							</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>
								 Two servers (up to 16 supported)
							</entry>
							<entry>
								<simplelist>
									<member>Each node includes the following hardware:</member>
									<member>Two network interfaces for:</member>
									<member>Client network access</member>
									<member>Fence device connection</member>
								</simplelist>
							</entry>
						</row>
						<row>
							<entry>
								 One network switch
							</entry>
							<entry>
								 A network switch enables the connection of multiple nodes to a network.
							</entry>
						</row>
						<row>
							<entry>
								 Three network cables (each node)
							</entry>
							<entry>
								 Two cables to connect each node to the redundant network switches and a cable to connect to the fence device.
							</entry>
						</row>
						<row>
							<entry>
								 Two RJ45 to DB9 crossover cables
							</entry>
							<entry>
								 RJ45 to DB9 crossover cables connect a serial port on each node to the Cyclades terminal server.
							</entry>
						</row>
						<row>
							<entry>
								 Two power switches
							</entry>
							<entry>
								 Power switches enable each node to power-cycle the other node before restarting its services. Two RJ45 Ethernet cables for a node are connected to each switch.
							</entry>
						</row>
						<row>
							<entry>
								 FlashDisk RAID Disk Array with dual controllers
							</entry>
							<entry>
								 Dual RAID controllers protect against disk and controller failure. The RAID controllers provide simultaneous access to all the logical units on the host ports.
							</entry>
						</row>
						<row>
							<entry>
								 Two HD68 SCSI cables
							</entry>
							<entry>
								 HD68 cables connect each host bus adapter to a RAID enclosure &#34;in&#34; port, creating two single-initiator SCSI buses.
							</entry>
						</row>
						<row>
							<entry>
								 Two terminators
							</entry>
							<entry>
								 Terminators connected to each &#34;out&#34; port on the RAID enclosure terminate both single-initiator SCSI buses.
							</entry>
						</row>
						<row>
							<entry>
								 Redundant UPS Systems
							</entry>
							<entry>
								 UPS systems provide a highly-available source of power. The power cables for the power switches and the RAID enclosure are connected to two UPS systems.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
			<indexterm>
				
				<primary>network hub</primary>
			</indexterm>
			<indexterm>
				
				<primary>network switch</primary>
			</indexterm>
			<indexterm>
				
				<primary>console switch</primary>
			</indexterm>
			<indexterm>
				
				<primary>hardware configuration</primary>
				<secondary>optional hardware</secondary>
			</indexterm>
			<para>
				 Cluster hardware configurations can also include other optional hardware components that are common in a computing environment. For example, a cluster can include a <firstterm>network switch</firstterm> or <firstterm>network hub</firstterm>, which enables the connection of the nodes to a network. A cluster may also include a <firstterm>console switch</firstterm>, which facilitates the management of multiple nodes and eliminates the need for separate monitors, mouses, and keyboards for each node. 
			</para>
			<indexterm>
				
				<primary>terminal server</primary>
			</indexterm>
			<indexterm>
				
				<primary>KVM (keyboard, video, mouse) switch</primary>
			</indexterm>
			<para>
				 One type of console switch is a <firstterm>terminal server</firstterm>, which enables connection to serial consoles and management of many nodes from one remote location. As a low-cost alternative, you can use a <firstterm>KVM</firstterm>(keyboard, video, and mouse) switch, which enables multiple nodes to share one keyboard, monitor, and mouse. A KVM switch is suitable for configurations in which access to a graphical user interface (GUI) to perform system management tasks is preferred. 
			</para>
			<para>
				 When choosing the computers to serve as cluster nodes, make sure that they provide enough PCI slots, network slots, and serial ports. For example, a no-single-point-of-failure configuration requires multiple bonded Ethernet ports. Refer to <xref linkend="s2-hardware-basic" /> for more information. 
			</para>
		</section>
		<section id="s2-hardware-pwrctrl">
			<title>Choosing the Type of Fence Device</title>
			<indexterm>
				
				<primary>cluster hardware</primary>
				<secondary>fence devices</secondary>
			</indexterm>
			<indexterm>
				
				<primary>fence devices</primary>
			</indexterm>
			<indexterm>
				
				<primary>fence devices</primary>
				<secondary>serial-attached</secondary>
			</indexterm>
			<indexterm>
				
				<primary>fence devices</primary>
				<secondary>network-attached</secondary>
			</indexterm>
			<indexterm>
				
				<primary>fence devices</primary>
				<secondary>watchdog timers</secondary>
			</indexterm>
			<indexterm>
				
				<primary>fence devices</primary>
				<secondary>watchdog timers</secondary>
				<tertiary>hardware-based</tertiary>
			</indexterm>
			<indexterm>
				
				<primary>fence devices</primary>
				<secondary>watchdog timers</secondary>
				<tertiary>software-based</tertiary>
			</indexterm>
			<indexterm>
				
				<primary>watchdog timers</primary>
				<secondary>hardware-based</secondary>
			</indexterm>
			<indexterm>
				
				<primary>watchdog timers</primary>
				<secondary>software-based</secondary>
			</indexterm>
			<important><title>Important</title>
			<para>
				 Use of a fencing method is an integral part of a production cluster environment. Configuration of a cluster without a fence device is not supported. 
			</para>
			</important>
			<para>
				&RH; Cluster supports several types of fencing methods, including network power switches, Fibre Channel fabric switches, and Integrated Power Management hardware. 
			</para>
			<para>
				 Ultimately, choosing the right type of fence device to deploy in a cluster environment depends on the data integrity requirements versus the cost and availability of external power switches. 
			</para>
		</section>
	</section>
	<indexterm>
		
		<primary>tables</primary>
		<secondary>cluster hardware</secondary>
	</indexterm>
	<indexterm>
		
		<primary>cluster hardware tables</primary>
	</indexterm>
	<section id="s1-hardware-clustertable">
		<title>Cluster Hardware Components</title>
		<para>
			 Use the following section to identify the hardware components required for the cluster configuration. 
		</para>
		<indexterm>
			
			<primary>tables</primary>
			<secondary>cluster node hardware</secondary>
		</indexterm>
		<indexterm>
			
			<primary>cluster node hardware table</primary>
		</indexterm>
		<table id="tb-hardware-clustersystem">
			<title>Cluster Node Hardware</title>
			<tgroup cols="4">
				<colspec colname="Hardware" colnum="1" colwidth="2*"></colspec><colspec colname="Quantity" colnum="2" colwidth="2*"></colspec><colspec colname="Description" colnum="3" colwidth="6*"></colspec><colspec colname="Required" colnum="4" colwidth="2*"></colspec><thead>
					<row>
						<entry>
							 Hardware
						</entry>
						<entry>
							 Quantity
						</entry>
						<entry>
							 Description
						</entry>
						<entry>
							 Required
						</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>
							 Cluster nodes
						</entry>
						<entry>
							16 (maximum supported)
						</entry>
						<entry>
							 Each node must provide enough PCI slots, network slots, and storage adapters for the cluster hardware configuration. Because attached storage devices must have the same device special file on each node, it is recommended that the nodes have symmetric I/O subsystems. It is also recommended that the processor speed and amount of system memory be adequate for the processes run on the cluster nodes. Refer to <xref linkend="s2-hardware-basic" /> for more information.
						</entry>
						<entry>
							 Yes
						</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
		<para>
			<xref linkend="tbl-hardware-fencedevs" /> includes several different types of fence devices. 
		</para>
		<para>
			 A single cluster requires only one type of power switch. 
		</para>
		<table id="tbl-hardware-fencedevs">
			<title>Fence Devices</title>
			<tgroup cols="3">
				<colspec colname="Type" colnum="1" colwidth="3*"></colspec><colspec colname="Description" colnum="2" colwidth="4*"></colspec><colspec colname="Models" colnum="3" colwidth="4*"></colspec><thead>
					<row>
						<entry>
							 Type
						</entry>
						<entry>
							 Description
						</entry>
						<entry>
							 Models
						</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>
							 Network-attached power switches.
						</entry>
						<entry>
							 Remote (LAN, Internet) fencing using RJ45 Ethernet connections and remote terminal access to the device.
						</entry>
						<entry>
							 APC MasterSwitch 92xx/96xx; WTI NPS-115/NPS-230, IPS-15, IPS-800/IPS-800-CE and TPS-2 
						</entry>
					</row>
					<row>
						<entry>
							 Fabric Switches.
						</entry>
						<entry>
							 Fence control interface integrated in several models of fabric switches used for Storage Area Networks (SANs). Used as a way to fence a failed node from accessing shared data.
						</entry>
						<entry>
							 Brocade Silkworm 2<replaceable>x</replaceable>00, McData Sphereon, Vixel 9200
						</entry>
					</row>
					<row>
						<entry>
							 Integrated Power Management Interfaces
						</entry>
						<entry>
							 Remote power management features in various brands of server systems; can be used as a fencing agent in cluster systems
						</entry>
						<entry>
							 HP Integrated Lights-out (iLO), IBM BladeCenter with firmware dated 7-22-04 or later
						</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
		<para>
			<xref linkend="tb-hardware-shareddisk" /> through <xref linkend="tb-hardware-ups" /> show a variety of hardware components for an administrator to choose from. An individual cluster does <emphasis>not</emphasis> require all of the components listed in these tables. 
		</para>
		<indexterm>
			
			<primary>tables</primary>
			<secondary>network hardware</secondary>
		</indexterm>
		<indexterm>
			
			<primary>network hardware table</primary>
		</indexterm>
		<table id="tb-hardware-network">
			<title>Network Hardware Table</title>
			<tgroup cols="4">
				<colspec colname="Hardware" colnum="1" colwidth="2*"></colspec><colspec colname="Quantity" colnum="2" colwidth="2*"></colspec><colspec colname="Description" colnum="3" colwidth="6*"></colspec><colspec colname="Required" colnum="4" colwidth="2*"></colspec><thead>
					<row>
						<entry>
							 Hardware
						</entry>
						<entry>
							 Quantity
						</entry>
						<entry>
							 Description
						</entry>
						<entry>
							 Required
						</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>
							 Network interface
						</entry>
						<entry>
							 One for each network connection
						</entry>
						<entry>
							 Each network connection requires a network interface installed in a node.
						</entry>
						<entry>
							 Yes
						</entry>
					</row>
					<row>
						<entry>
							 Network switch or hub
						</entry>
						<entry>
							 One
						</entry>
						<entry>
							 A network switch or hub allows connection of multiple nodes to a network.
						</entry>
						<entry>
							 Yes
						</entry>
					</row>
					<row>
						<entry>
							 Network cable
						</entry>
						<entry>
							 One for each network interface
						</entry>
						<entry>
							 A conventional network cable, such as a cable with an RJ45 connector, connects each network interface to a network switch or a network hub.
						</entry>
						<entry>
							 Yes
						</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
		<indexterm>
			
			<primary>tables</primary>
			<secondary>shared disk storage hardware</secondary>
		</indexterm>
		<indexterm>
			
			<primary>shared disk storage hardware table</primary>
		</indexterm>
		<table id="tb-hardware-shareddisk">
			<title>Shared Disk Storage Hardware Table</title>
			<tgroup cols="4">
				<colspec colname="Hardware" colnum="1" colwidth="2*"></colspec><colspec colname="Quantity" colnum="2" colwidth="2*"></colspec><colspec colname="Description" colnum="3" colwidth="6*"></colspec><colspec colname="Required" colnum="4" colwidth="2*"></colspec><thead>
					<row>
						<entry>
							 Hardware
						</entry>
						<entry>
							 Quantity
						</entry>
						<entry>
							 Description
						</entry>
						<entry>
							 Required
						</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>
							 Host bus adapter
						</entry>
						<entry>
							 One per node
						</entry>
						<entry>
							<simplelist>
								<member>To connect to shared disk storage, install either a parallel SCSI or a Fibre Channel host bus adapter in a PCI slot in each cluster node.</member>
								<member>For parallel SCSI, use a low voltage differential (LVD) host bus adapter. Adapters have either HD68 or VHDCI connectors.</member>
							</simplelist>
						</entry>
						<entry>
							 Yes
						</entry>
					</row>
					<row>
						<entry>
							 External disk storage enclosure
						</entry>
						<entry>
							 At least one
						</entry>
						<entry>
							<simplelist>
								<member>Use Fibre Channel or single-initiator parallel SCSI to connect the cluster nodes to a single or dual-controller RAID array. To use single-initiator buses, a RAID controller must have multiple host ports and provide simultaneous access to all the logical units on the host ports. To use a dual-controller RAID array, a logical unit must fail over from one controller to the other in a way that is transparent to the operating system.</member>
								<member>SCSI RAID arrays that provide simultaneous access to all logical units on the host ports are recommended.</member>
								<member>To ensure symmetry of device IDs and LUNs, many RAID arrays with dual redundant controllers must be configured in an active/passive mode.</member>
								<member>Refer to <xref linkend="ap-hwinfo" />for more information.</member>
							</simplelist>
						</entry>
						<entry>
							 Yes
						</entry>
					</row>
					<row>
						<entry>
							 SCSI cable
						</entry>
						<entry>
							 One per node
						</entry>
						<entry>
							 SCSI cables with 68 pins connect each host bus adapter to a storage enclosure port. Cables have either HD68 or VHDCI connectors. Cables vary based on adapter type.
						</entry>
						<entry>
							 Only for parallel SCSI configurations
						</entry>
					</row>
					<row>
						<entry>
							 SCSI terminator
						</entry>
						<entry>
							 As required by hardware configuration
						</entry>
						<entry>
							 For a RAID storage enclosure that uses &#34;out&#34; ports (such as FlashDisk RAID Disk Array) and is connected to single-initiator SCSI buses, connect terminators to the &#34;out&#34; ports to terminate the buses.
						</entry>
						<entry>
							 Only for parallel SCSI configurations and only as necessary for termination
						</entry>
					</row>
					<row>
						<entry>
							 Fibre Channel hub or switch
						</entry>
						<entry>
							 One or two
						</entry>
						<entry>
							 A Fibre Channel hub or switch may be required.
						</entry>
						<entry>
							 Only for some Fibre Channel configurations
						</entry>
					</row>
					<row>
						<entry>
							 Fibre Channel cable
						</entry>
						<entry>
							 As required by hardware configuration
						</entry>
						<entry>
							 A Fibre Channel cable connects a host bus adapter to a storage enclosure port, a Fibre Channel hub, or a Fibre Channel switch. If a hub or switch is used, additional cables are needed to connect the hub or switch to the storage adapter ports.
						</entry>
						<entry>
							 Only for Fibre Channel configurations
						</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
		<indexterm>
			
			<primary>tables</primary>
			<secondary>UPS system hardware</secondary>
		</indexterm>
		<indexterm>
			
			<primary>UPS system hardware table</primary>
		</indexterm>
		<table id="tb-hardware-ups">
			<title>UPS System Hardware Table</title>
			<tgroup cols="4">
				<colspec colname="Hardware" colnum="1" colwidth="2*"></colspec><colspec colname="Quantity" colnum="2" colwidth="2*"></colspec><colspec colname="Description" colnum="3" colwidth="6*"></colspec><colspec colname="Required" colnum="4" colwidth="2*"></colspec><thead>
					<row>
						<entry>
							 Hardware
						</entry>
						<entry>
							 Quantity
						</entry>
						<entry>
							 Description
						</entry>
						<entry>
							 Required
						</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>
							 UPS system
						</entry>
						<entry>
							 One or more
						</entry>
						<entry>
							<firstterm>Uninterruptible power supply</firstterm>							(UPS) systems protect against downtime if a power outage occurs. UPS systems are highly recommended for cluster operation. Connect the power cables for the shared storage enclosure and both power switches to redundant UPS systems. Note that a UPS system must be able to provide voltage for an adequate period of time, and should be connected to its own power circuit.
						</entry>
						<entry>
							 Strongly recommended for availability
						</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
		<indexterm>
			
			<primary>tables</primary>
			<secondary>console switch hardware</secondary>
		</indexterm>
		<indexterm>
			
			<primary>console switch hardware table</primary>
		</indexterm>
		<table id="tb-hardware-console">
			<title>Console Switch Hardware Table</title>
			<tgroup cols="4">
				<colspec colname="Hardware" colnum="1" colwidth="2*"></colspec><colspec colname="Quantity" colnum="2" colwidth="2*"></colspec><colspec colname="Description" colnum="3" colwidth="6*"></colspec><colspec colname="Required" colnum="4" colwidth="2*"></colspec><thead>
					<row>
						<entry>
							 Hardware
						</entry>
						<entry>
							 Quantity
						</entry>
						<entry>
							 Description
						</entry>
						<entry>
							 Required
						</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>
							 Terminal server
						</entry>
						<entry>
							 One
						</entry>
						<entry>
							 A terminal server enables you to manage many nodes remotely. 
						</entry>
						<entry>
							 No
						</entry>
					</row>
					<row>
						<entry>
							 KVM switch
						</entry>
						<entry>
							 One
						</entry>
						<entry>
							 A KVM switch enables multiple nodes to share one keyboard, monitor, and mouse. Cables for connecting nodes to the switch depend on the type of KVM switch.
						</entry>
						<entry>
							 No
						</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
	</section>
	<section id="s1-hardware-cluster">
		<title>Setting Up the Nodes</title>
		<indexterm>
			
			<primary>nodes</primary>
			<secondary>setting up</secondary>
		</indexterm>
		<para>
			 After identifying the cluster hardware components described in <xref linkend="s1-hardware-choosing" />, set up the basic cluster hardware and connect the nodes to the optional console switch and network switch or hub. Follow these steps: 
		</para>
		<orderedlist>
			<listitem>
				<para>
					 In all nodes, install the required network adapters and host bus adapters. Refer to <xref linkend="s2-hardware-basic" /> for more information about performing this task. 
				</para>
			</listitem>
			<listitem>
				<para>
					 Set up the optional console switch and connect it to each node. Refer to <xref linkend="s2-hardware-consolesetup" /> for more information about performing this task. 
				</para>
				<para>
					 If a console switch is not used, then connect each node to a console terminal. 
				</para>
			</listitem>
			<listitem>
				<para>
					 Set up the network switch or hub and use network cables to connect it to the nodes and the terminal server (if applicable). Refer to <xref linkend="s2-hardware-networksetup" /> for more information about performing this task. 
				</para>
			</listitem>
		</orderedlist>
		<para>
			 After performing the previous tasks, install &PROD; as described in <xref linkend="s1-hardware-linux" />. 
		</para>
		<section id="s2-hardware-basic">
			<title>Installing the Basic Cluster Hardware</title>
			<indexterm>
				
				<primary>hardware</primary>
				<secondary>installing basic cluster hardware</secondary>
			</indexterm>
			<indexterm>
				
				<primary>installing basic cluster hardware</primary>
			</indexterm>
			<para>
				 Nodes must provide the CPU processing power and memory required by applications. 
			</para>
			<para>
				 In addition, nodes must be able to accommodate the SCSI or Fibre Channel adapters, network interfaces, and serial ports that the hardware configuration requires. Systems have a limited number of pre-installed serial and network ports and PCI expansion slots. <xref linkend="tb-hardware-basic" /> helps determine how much capacity the employed node systems require. 
			</para>
			<indexterm>
				
				<primary>tables</primary>
				<secondary>installing the basic cluster hardware</secondary>
			</indexterm>
			<indexterm>
				
				<primary>installing the basic cluster hardware</primary>
			</indexterm>
			<table id="tb-hardware-basic">
				<title>Installing the Basic Cluster Hardware</title>
				<tgroup cols="4">
					<colspec colname="Hardware" colnum="1" colwidth="5*"></colspec><colspec colname="SerialPorts" colnum="2" colwidth="1*"></colspec><colspec colname="NetworkSlots" colnum="3" colwidth="1*"></colspec><colspec colname="PCIslots" colnum="4" colwidth="1*"></colspec><thead>
						<row>
							<entry>
								 Cluster Hardware Component
							</entry>
							<entry>
								 Serial Ports
							</entry>
							<entry>
								 Ethernet Ports
							</entry>
							<entry>
								 PCI Slots
							</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>
								 SCSI or Fibre Channel adapter to shared disk storage
							</entry>
							<entry>
							</entry>
							<entry>
							</entry>
							<entry>
								 One for each bus adapter
							</entry>
						</row>
						<row>
							<entry>
								 Network connection for client access and Ethernet heartbeat pings 
							</entry>
							<entry>
							</entry>
							<entry>
								 One for each network connection
							</entry>
							<entry>
							</entry>
						</row>
						<row>
							<entry>
								 Point-to-point Ethernet connection for 2-node clusters (optional)
							</entry>
							<entry>
							</entry>
							<entry>
								 One for each connection
							</entry>
							<entry>
							</entry>
						</row>
						<row>
							<entry>
								 Terminal server connection (optional)
							</entry>
							<entry>
								 One
							</entry>
							<entry>
							</entry>
							<entry>
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
			<para>
				 Most systems come with at least one serial port. If a system has graphics display capability, it is possible to use the serial console port for a power switch connection. To expand your serial port capacity, use multi-port serial PCI cards. For multiple-node clusters, use a network power switch. 
			</para>
			<para>
				 Also, ensure that local system disks are not on the same SCSI bus as the shared disks. For example, use two-channel SCSI adapters, such as the Adaptec 39160-series cards, and put the internal devices on one channel and the shared disks on the other channel. Using multiple SCSI cards is also possible. 
			</para>
			<para>
				 Refer to the system documentation supplied by the vendor for detailed installation information. Refer to <xref linkend="ap-hwinfo" /> for hardware-specific information about using host bus adapters in a cluster. 
			</para>
		</section>
		<section id="s2-hardware-storagesetup">
			<title>Shared Storage considerations</title>
			<indexterm>
				
				<primary>shared storage</primary>
				<secondary>setting up</secondary>
			</indexterm>
			<indexterm>
				
				<primary>shared storage</primary>
				<secondary>considerations</secondary>
			</indexterm>
			<para>
				 In a cluster, shared disks can be used to store cluster service data. Because this storage must be available to all nodes running the cluster service configured to use the storage, it cannot be located on disks that depend on the availability of any one node. 
			</para>
			<para>
				 There are some factors to consider when setting up shared disk storage in a cluster: 
			</para>
			<itemizedlist>
				<listitem>
					<para>
						 It is recommended to use a clustered file system such as &RHGFS; to configure &RH; Cluster storage resources, as it offers shared storage that is suited for high-availability cluster services. For more information about installing and configuring &RHGFS;, refer to the <citetitle>&RHGFSG;</citetitle>. 
					</para>
				</listitem>
				<listitem>
					<para>
						 Whether you are using &RHGFS;, local, or remote (for example, NFS) storage, it is <emphasis>strongly recommended</emphasis> that you connect any storage systems or enclosures to redundant UPS systems for a highly-available source of power. Refer to <xref linkend="s2-hardware-upscfg" /> for more information. 
					</para>
				</listitem>
				<listitem>
					<para>
						 The use of software RAID or <firstterm>Logical Volume Management</firstterm>(<acronym>LVM</acronym>) for shared storage is not supported. This is because these products do not coordinate access to shared storage from multiple hosts. Software RAID or LVM may be used on non-shared storage on cluster nodes (for example, boot and system partitions, and other file systems that are not associated with any cluster services). 
					</para>
					<para>
						 An exception to this rule is <firstterm>CLVM</firstterm>, the daemon and library that supports clustering of LVM2. CLVM allows administrators to configure shared storage for use as a resource in cluster services when used in conjunction with the CMAN cluster manager and the <firstterm>Distributed Lock Manager</firstterm>(DLM) mechanism for prevention of simultaneous node access to data and possible corruption. In addition, CLVM works with GULM as its cluster manager and lock manager. 
					</para>
				</listitem>
				<listitem>
					<para>
						 For remote file systems such as NFS, you may use gigabit Ethernet for improved bandwidth over 10/100 Ethernet connections. Consider redundant links or channel bonding for improved remote file system availability. Refer to <xref linkend="s2-hardware-ethbond" /> for more information. 
					</para>
				</listitem>
				<listitem>
					<para>
						 Multi-initiator SCSI configurations are not supported due to the difficulty in obtaining proper bus termination. Refer to <xref linkend="ap-hwinfo" /> for more information about configuring attached storage. 
					</para>
				</listitem>
				<listitem>
					<para>
						<indexterm>
							
							<primary>shared storage</primary>
						</indexterm>
						 A shared partition can be used by only one cluster service. 
					</para>
				</listitem>
				<listitem>
					<para>
						 Do not include any file systems used as a resource for a cluster service in the node's local <filename>/etc/fstab</filename> files, because the cluster software must control the mounting and unmounting of service file systems. 
					</para>
				</listitem>
				<listitem>
					<para>
						 For optimal performance of shared file systems, make sure to specify a 4 KB block size with the <command>mke2fs -b</command> command. A smaller block size can cause long <command>fsck</command> times. Refer to <xref linkend="s3-hardware-createfs" />. 
					</para>
				</listitem>
			</itemizedlist>
			<para>
				 After setting up the shared disk storage hardware, partition the disks and create file systems on the partitions. Refer to <xref linkend="s3-hardware-partdisks" />, and <xref linkend="s3-hardware-createfs" /> for more information on configuring disks. 
			</para>
		</section>
		<section id="s2-hardware-consolesetup">
			<title>Setting Up a Console Switch</title>
			<indexterm>
				
				<primary>console switch</primary>
				<secondary>setting up</secondary>
			</indexterm>
			<para>
				 Although a console switch is not required for cluster operation, it can be used to facilitate node management and eliminate the need for separate monitors, mouses, and keyboards for each cluster node. There are several types of console switches. 
			</para>
			<para>
				 For example, a terminal server enables connection to serial consoles and management of many nodes from a remote location. For a low-cost alternative, use a KVM (keyboard, video, and mouse) switch, which enables multiple nodes to share one keyboard, monitor, and mouse. A KVM switch is suitable for configurations in which GUI access to perform system management tasks is preferred. 
			</para>
			<para>
				 Set up the console switch according to the documentation provided by the vendor. 
			</para>
			<para>
				 After the console switch has been set up, connect it to each cluster node. The cables used depend on the type of console switch. For example, a Cyclades terminal server uses RJ45 to DB9 crossover cables to connect a serial port on each node to the terminal server. 
			</para>
		</section>
		<section id="s2-hardware-networksetup">
			<title>Setting Up a Network Switch or Hub</title>
			<para>
				 A network switch or hub, although not required for operating a two-node cluster, can be used to facilitate cluster and client system network operations. Clusters of more than two nodes require a switch or hub. 
			</para>
			<para>
				 Set up a network switch or hub according to the documentation provided by the vendor. 
			</para>
			<para>
				 After setting up the network switch or hub, connect it to each node by using conventional network cables. A terminal server, if used, is connected to the network switch or hub through a network cable. 
			</para>
		</section>
	</section>
	<section id="s1-hardware-linux">
		<title>Installing and Configuring &PROD;</title>
		<indexterm>
			
			<primary>&PROD;</primary>
			<secondary>installation and configuration</secondary>
		</indexterm>
		<indexterm>
			
			<primary>installation</primary>
			<secondary>&PROD;</secondary>
		</indexterm>
		<indexterm>
			
			<primary>configuration</primary>
			<secondary>&PROD;</secondary>
		</indexterm>
		<para>
			 After the setup of basic cluster hardware, proceed with installation of &PROD; on each node and ensure that all systems recognize the connected devices. Follow these steps: 
		</para>
		<orderedlist>
			<listitem>
				<para>
					 Install &PROD; on all cluster nodes. Refer to <citetitle>&RHELIG;</citetitle> for instructions. 
				</para>
				<para>
					 In addition, when installing &PROD;, it is <emphasis>strongly recommended</emphasis> to do the following: 
				</para>
				<itemizedlist>
					<listitem>
						<para>
							 Gather the IP addresses for the nodes and for the bonded Ethernet ports, before installing &PROD;. Note that the IP addresses for the bonded Ethernet ports can be private IP addresses, (for example, 10.<replaceable>x.x.x</replaceable>). 
						</para>
					</listitem>
					<listitem>
						<para>
							 Do not place local file systems (such as <filename>/</filename>, <filename>/etc</filename>, <filename>/tmp</filename>, and <filename>/var</filename>) on shared disks or on the same SCSI bus as shared disks. This helps prevent the other cluster nodes from accidentally mounting these file systems, and also reserves the limited number of SCSI identification numbers on a bus for cluster disks. 
						</para>
					</listitem>
					<listitem>
						<para>
							 Place <filename>/tmp</filename> and <filename>/var</filename> on different file systems. This may improve node performance. 
						</para>
					</listitem>
					<listitem>
						<para>
							 When a node boots, be sure that the node detects the disk devices in the same order in which they were detected during the &PROD; installation. If the devices are not detected in the same order, the node may not boot. 
						</para>
					</listitem>
					<listitem>
						<para>
							 When using certain RAID storage configured with Logical Unit Numbers (<acronym>LUN</acronym> s) greater than zero, it may be necessary to enable LUN support by adding the following to <filename>/etc/modprobe.conf</filename>: 
						</para>
						<screen><computeroutput>options scsi_mod max_scsi_luns=255 </computeroutput></screen>
					</listitem>
				</itemizedlist>
			</listitem>
			<listitem>
				<para>
					 Reboot the nodes. 
				</para>
			</listitem>
			<listitem>
				<para>
					 When using a terminal server, configure &PROD; to send console messages to the console port. 
				</para>
			</listitem>
			<listitem>
				<para>
					 Edit the <filename>/etc/hosts</filename> file on each cluster node and include the IP addresses used in the cluster or ensure that the addresses are in DNS. Refer to <xref linkend="s2-hardware-etchosts" /> for more information about performing this task. 
				</para>
			</listitem>
			<listitem>
				<para>
					 Decrease the alternate kernel boot timeout limit to reduce boot time for nodes. Refer to <xref linkend="s2-hardware-timeout" /> for more information about performing this task. 
				</para>
			</listitem>
			<listitem>
				<para>
					 Ensure that no login (or <command>getty</command>) programs are associated with the serial ports that are being used for the remote power switch connection (if applicable). To perform this task, edit the <filename>/etc/inittab</filename> file and use a hash symbol (<symbol>#</symbol>) to comment out the entries that correspond to the serial ports used for the remote power switch. Then, invoke the <command>init q</command> command. 
				</para>
			</listitem>
			<listitem>
				<para>
					 Verify that all systems detect all the installed hardware: 
				</para>
				<itemizedlist>
					<listitem>
						<para>
							 Use the <command>dmesg</command> command to display the console startup messages. Refer to <xref linkend="s2-hardware-consolemsg" /> for more information about performing this task. 
						</para>
					</listitem>
					<listitem>
						<para>
							 Use the <command>cat /proc/devices</command> command to display the devices configured in the kernel. Refer to <xref linkend="s2-hardware-kdevices" /> for more information about performing this task. 
						</para>
					</listitem>
				</itemizedlist>
			</listitem>
			<listitem>
				<para>
					 Verify that the nodes can communicate over all the network interfaces by using the <command>ping</command> command to send test packets from one node to another. 
				</para>
			</listitem>
			<listitem>
				<para>
					 If intending to configure Samba services, verify that the required RPM packages for Samba services are installed. 
				</para>
			</listitem>
		</orderedlist>
		<section id="s2-hardware-etchosts">
			<title>Editing the <filename>/etc/hosts</filename>File</title>
			<indexterm>
				
				<primary>/etc/hosts</primary>
				<secondary>editing</secondary>
			</indexterm>
			<para>
				 The <filename>/etc/hosts</filename> file contains the IP address-to-hostname translation table. The <filename>/etc/hosts</filename> file on each node must contain entries for IP addresses and associated hostnames for all cluster nodes. 
			</para>
			<para>
				 As an alternative to the <filename>/etc/hosts</filename> file, name services such as DNS or NIS can be used to define the host names used by a cluster. However, to limit the number of dependencies and optimize availability, it is strongly recommended to use the <filename>/etc/hosts</filename> file to define IP addresses for cluster network interfaces. 
			</para>
			<para>
				 The following is an example of an <filename>/etc/hosts</filename> file on a node of a cluster that does not use DNS-assigned hostnames: 
			</para>
			<screen><computeroutput>127.0.0.1 localhost.localdomain localhost 192.168.1.81 node1.example.com node1 193.186.1.82 node2.example.com node2 193.186.1.83 node3.example.com node3 </computeroutput></screen><para>
				 The previous example shows the IP addresses and hostnames for three nodes (<firstterm>node1</firstterm>, <firstterm>node2</firstterm>, and <firstterm>node3</firstterm>), 
			</para>
			<important><title>Important</title>
			<para>
				 Do <emphasis>not</emphasis> assign the node hostname to the localhost (127.0.0.1) address, as this causes issues with the CMAN cluster management system. 
			</para>
			</important>
			<para>
				 Verify correct formatting of the local host entry in the <filename>/etc/hosts</filename> file to ensure that it does not include non-local systems in the entry for the local host. An example of an incorrect local host entry that includes a non-local system (<firstterm>server1</firstterm>) is shown next: 
			</para>
			<screen><computeroutput>127.0.0.1 localhost.localdomain localhost server1 </computeroutput></screen><para>
				 An Ethernet connection may not operate properly if the format of the <filename>/etc/hosts</filename> file is not correct. Check the <filename>/etc/hosts</filename> file and correct the file format by removing non-local systems from the local host entry, if necessary. 
			</para>
			<para>
				 Note that each network adapter must be configured with the appropriate IP address and netmask. 
			</para>
			<para>
				 The following example shows a portion of the output from the <command>/sbin/ip addr list</command> command on a cluster node: 
			</para>
			<screen><computeroutput>2: eth0: &lt;BROADCAST,MULTICAST,UP&gt; mtu 1356 qdisc pfifo_fast qlen 1000 link/ether 00:05:5d:9a:d8:91 brd ff:ff:ff:ff:ff:ff inet 10.11.4.31/22 brd 10.11.7.255 scope global eth0 inet6 fe80::205:5dff:fe9a:d891/64 scope link valid_lft forever preferred_lft forever </computeroutput></screen><para>
				 You may also add the IP addresses for the cluster nodes to your DNS server. Refer to the <citetitle>&RHELSAG;</citetitle> for information on configuring DNS, or consult your network administrator. 
			</para>
		</section>
		<section id="s2-hardware-timeout">
			<title>Decreasing the Kernel Boot Timeout Limit</title>
			<indexterm>
				
				<primary>kernel</primary>
				<secondary>decreasing kernel boot timeout limit</secondary>
			</indexterm>
			<indexterm>
				
				<primary>Kernel Boot Timeout Limit</primary>
				<secondary>decreasing</secondary>
			</indexterm>
			<para>
				 It is possible to reduce the boot time for a node by decreasing the kernel boot timeout limit. During the &PROD; boot sequence, the boot loader allows for specifying an alternate kernel to boot. The default timeout limit for specifying a kernel is ten seconds. 
			</para>
			<para>
				 To modify the kernel boot timeout limit for a node, edit the appropriate files as follows: 
			</para>
			<para>
				 When using the GRUB boot loader, the timeout parameter in <filename>/boot/grub/grub.conf</filename> should be modified to specify the appropriate number of seconds for the <parameter>timeout</parameter> parameter. To set this interval to 3 seconds, edit the parameter to the following: 
			</para>
			<screen><computeroutput>timeout = 3 </computeroutput></screen><para>
				 When using the LILO or ELILO boot loaders, edit the <filename>/etc/lilo.conf</filename> file (on x86 systems) or the <filename>elilo.conf</filename> file (on Itanium systems) and specify the desired value (in tenths of a second) for the <parameter>timeout</parameter> parameter. The following example sets the timeout limit to three seconds: 
			</para>
			<screen><computeroutput>timeout = 30 </computeroutput></screen><para>
				 To apply any changes made to the <filename>/etc/lilo.conf</filename> file, invoke the <command>/sbin/lilo</command> command. 
			</para>
			<para>
				 On an Itanium system, to apply any changes made to the <filename>/boot/efi/efi/redhat/elilo.conf</filename> file, invoke the <command>/sbin/elilo</command> command. 
			</para>
		</section>
		<section id="s2-hardware-consolemsg">
			<title>Displaying Console Startup Messages</title>
			<indexterm>
				
				<primary>console startup messages</primary>
				<secondary>displaying</secondary>
			</indexterm>
			<indexterm>
				
				<primary>displaying console startup messages</primary>
			</indexterm>
			<para>
				 Use the <command>dmesg</command> command to display the console startup messages. Refer to the <citerefentry><refentrytitle>dmesg</refentrytitle><manvolnum>8</manvolnum></citerefentry> man page for more information. 
			</para>
			<para>
				 The following example of output from the <command>dmesg</command> command shows that two external SCSI buses and nine disks were detected on the node. (Lines with backslashes display as one line on most screens): 
			</para>
			<screen><computeroutput>May 22 14:02:10 storage3 kernel: scsi0 : Adaptec AHA274x/284x/294x &#92; (EISA/VLB/PCI-Fast SCSI) 5.1.28/3.2.4 May 22 14:02:10 storage3 kernel: May 22 14:02:10 storage3 kernel: scsi1 : Adaptec AHA274x/284x/294x &#92; (EISA/VLB/PCI-Fast SCSI) 5.1.28/3.2.4 May 22 14:02:10 storage3 kernel: May 22 14:02:10 storage3 kernel: scsi : 2 hosts. May 22 14:02:11 storage3 kernel: Vendor: SEAGATE Model: ST39236LW Rev: 0004 May 22 14:02:11 storage3 kernel: Detected scsi disk sda at scsi0, channel 0, id 0, lun 0 May 22 14:02:11 storage3 kernel: Vendor: SEAGATE Model: ST318203LC Rev: 0001 May 22 14:02:11 storage3 kernel: Detected scsi disk sdb at scsi1, channel 0, id 0, lun 0 May 22 14:02:11 storage3 kernel: Vendor: SEAGATE Model: ST318203LC Rev: 0001 May 22 14:02:11 storage3 kernel: Detected scsi disk sdc at scsi1, channel 0, id 1, lun 0 May 22 14:02:11 storage3 kernel: Vendor: SEAGATE Model: ST318203LC Rev: 0001 May 22 14:02:11 storage3 kernel: Detected scsi disk sdd at scsi1, channel 0, id 2, lun 0 May 22 14:02:11 storage3 kernel: Vendor: SEAGATE Model: ST318203LC Rev: 0001 May 22 14:02:11 storage3 kernel: Detected scsi disk sde at scsi1, channel 0, id 3, lun 0 May 22 14:02:11 storage3 kernel: Vendor: SEAGATE Model: ST318203LC Rev: 0001 May 22 14:02:11 storage3 kernel: Detected scsi disk sdf at scsi1, channel 0, id 8, lun 0 May 22 14:02:11 storage3 kernel: Vendor: SEAGATE Model: ST318203LC Rev: 0001 May 22 14:02:11 storage3 kernel: Detected scsi disk sdg at scsi1, channel 0, id 9, lun 0 May 22 14:02:11 storage3 kernel: Vendor: SEAGATE Model: ST318203LC Rev: 0001 May 22 14:02:11 storage3 kernel: Detected scsi disk sdh at scsi1, channel 0, id 10, lun 0 May 22 14:02:11 storage3 kernel: Vendor: SEAGATE Model: ST318203LC Rev: 0001 May 22 14:02:11 storage3 kernel: Detected scsi disk sdi at scsi1, channel 0, id 11, lun 0 May 22 14:02:11 storage3 kernel: Vendor: Dell Model: 8 BAY U2W CU Rev: 0205 May 22 14:02:11 storage3 kernel: Type: Processor &#92; ANSI SCSI revision: 03 May 22 14:02:11 storage3 kernel: scsi1 : channel 0 target 15 lun 1 request sense &#92; failed, performing reset. May 22 14:02:11 storage3 kernel: SCSI bus is being reset for host 1 channel 0. May 22 14:02:11 storage3 kernel: scsi : detected 9 SCSI disks total. </computeroutput></screen><para>
				 The following example of the <command>dmesg</command> command output shows that a quad Ethernet card was detected on the node: 
			</para>
			<screen><computeroutput>May 22 14:02:11 storage3 kernel: 3c59x.c:v0.99H 11/17/98 Donald Becker May 22 14:02:11 storage3 kernel: tulip.c:v0.91g-ppc 7/16/99 May 22 14:02:11 storage3 kernel: eth0: Digital DS21140 Tulip rev 34 at 0x9800, &#92; 00:00:BC:11:76:93, IRQ 5. May 22 14:02:12 storage3 kernel: eth1: Digital DS21140 Tulip rev 34 at 0x9400, &#92; 00:00:BC:11:76:92, IRQ 9. May 22 14:02:12 storage3 kernel: eth2: Digital DS21140 Tulip rev 34 at 0x9000, &#92; 00:00:BC:11:76:91, IRQ 11. May 22 14:02:12 storage3 kernel: eth3: Digital DS21140 Tulip rev 34 at 0x8800, &#92; 00:00:BC:11:76:90, IRQ 10. </computeroutput></screen>
		</section>
		<section id="s2-hardware-kdevices">
			<title>Displaying Devices Configured in the Kernel</title>
			<indexterm>
				
				<primary>kernel</primary>
				<secondary>displaying configured devices</secondary>
			</indexterm>
			<indexterm>
				
				<primary>displaying devices configured in the kernel</primary>
			</indexterm>
			<para>
				 To be sure that the installed devices (such as network interfaces), are configured in the kernel, use the <command>cat /proc/devices</command> command on each node. For example: 
			</para>
			<screen><computeroutput>Character devices: 1 mem 4 /dev/vc/0 4 tty 4 ttyS 5 /dev/tty 5 /dev/console 5 /dev/ptmx 6 lp 7 vcs 10 misc 13 input 14 sound 29 fb 89 i2c 116 alsa 128 ptm 136 pts 171 ieee1394 180 usb 216 rfcomm 226 drm 254 pcmcia Block devices: 1 ramdisk 2 fd 3 ide0 8 sd 9 md 65 sd 66 sd 67 sd 68 sd 69 sd 70 sd 71 sd 128 sd 129 sd 130 sd 131 sd 132 sd 133 sd 134 sd 135 sd 253 device-mapper </computeroutput></screen><para>
				 The previous example shows: 
			</para>
			<itemizedlist>
				<listitem>
					<para>
						 Onboard serial ports (<computeroutput>ttyS</computeroutput>) 
					</para>
				</listitem>
				<listitem>
					<para>
						 USB devices (<computeroutput>usb</computeroutput>) 
					</para>
				</listitem>
				<listitem>
					<para>
						 SCSI devices (<computeroutput>sd</computeroutput>) 
					</para>
				</listitem>
			</itemizedlist>
		</section>
	</section>
	<section id="s1-hardware-connect">
		<title>Setting Up and Connecting the Cluster Hardware</title>
		<indexterm>
			
			<primary>cluster hardware</primary>
			<secondary>setting up</secondary>
		</indexterm>
		<indexterm>
			
			<primary>cluster hardware</primary>
			<secondary>connecting</secondary>
		</indexterm>
		<para>
			 After installing &PROD;, set up the cluster hardware components and verify the installation to ensure that the nodes recognize all the connected devices. Note that the exact steps for setting up the hardware depend on the type of configuration. Refer to <xref linkend="s1-hardware-choosing" /> for more information about cluster configurations. 
		</para>
		<para>
			 To set up the cluster hardware, follow these steps: 
		</para>
		<orderedlist>
			<listitem>
				<para>
					 Shut down the nodes and disconnect them from their power source. 
				</para>
			</listitem>
			<listitem>
				<para>
					 When using power switches, set up the switches and connect each node to a power switch. Refer to <xref linkend="s2-hardware-pwrcfg" /> for more information. 
				</para>
				<para>
					 In addition, it is recommended to connect each power switch (or each node's power cord if not using power switches) to a different UPS system. Refer to <xref linkend="s2-hardware-upscfg" /> for information about using optional UPS systems. 
				</para>
			</listitem>
			<listitem>
				<para>
					 Set up shared disk storage according to the vendor instructions and connect the nodes to the external storage enclosure. Refer to <xref linkend="s2-hardware-storagesetup" />. 
				</para>
				<para>
					 In addition, it is recommended to connect the storage enclosure to redundant UPS systems. Refer to <xref linkend="s2-hardware-upscfg" /> for more information about using optional UPS systems. 
				</para>
			</listitem>
			<listitem>
				<para>
					 Turn on power to the hardware, and boot each cluster node. During the boot-up process, enter the BIOS utility to modify the node setup, as follows: 
				</para>
				<itemizedlist>
					<listitem>
						<para>
							 Ensure that the SCSI identification number used by the host bus adapter is unique for the SCSI bus it is attached to. Refer to <xref linkend="s2-hwinfo-num" /> for more information about performing this task. 
						</para>
					</listitem>
					<listitem>
						<para>
							 Enable or disable the onboard termination for each host bus adapter, as required by the storage configuration. Refer to <xref linkend="s2-hwinfo-scsibus" /> for more information about performing this task. 
						</para>
					</listitem>
					<listitem>
						<para>
							 Enable the node to automatically boot when it is powered on. 
						</para>
					</listitem>
				</itemizedlist>
			</listitem>
			<listitem>
				<para>
					 Exit from the BIOS utility, and continue to boot each node. Examine the startup messages to verify that the &PROD; kernel has been configured and can recognize the full set of shared disks. Use the <command>dmesg</command> command to display console startup messages. Refer to <xref linkend="s2-hardware-consolemsg" /> for more information about using the <command>dmesg</command> command. 
				</para>
			</listitem>
			<listitem>
				<para>
					 Set up the bonded Ethernet channels, if applicable. Refer to <xref linkend="s2-hardware-ethbond" /> for more information. 
				</para>
			</listitem>
			<listitem>
				<para>
					 Run the <command>ping</command> command to verify packet transmission between <emphasis>all</emphasis> cluster nodes. 
				</para>
			</listitem>
		</orderedlist>
		<section id="s2-hardware-ethbond">
			<title>Configuring Ethernet Channel Bonding</title>
			<indexterm>
				
				<primary>Ethernet channel bonding</primary>
				<secondary>configuring</secondary>
			</indexterm>
			<indexterm>
				
				<primary>channel bonding</primary><see>Ethernet bonding</see>
			</indexterm>
			<para>
				 Ethernet channel bonding in a no-single-point-of-failure cluster system allows for a fault tolerant network connection by combining two Ethernet devices into one virtual device. The resulting channel bonded interface ensures that in the event that one Ethernet device fails, the other device will become active. This type of channel bonding, called an <firstterm>active-backup</firstterm> policy allows connection of both bonded devices to one switch or can allow each Ethernet device to be connected to separate hubs or switches, which eliminates the single point of failure in the network hub/switch. 
			</para>
			<para>
				 Channel bonding requires each cluster node to have two Ethernet devices installed. When it is loaded, the bonding module uses the MAC address of the first enslaved network device and assigns that MAC address to the other network device if the first device fails link detection. 
			</para>
			<para>
				 To configure two network devices for channel bonding, perform the following: 
			</para>
			<orderedlist>
				<listitem>
					<para>
						 Create a bonding devices in <filename>/etc/modprobe.conf</filename>. For example: 
					</para>
					<screen><computeroutput>alias bond0 bonding options bonding miimon=100 mode=1 </computeroutput></screen><para>
						 This loads the bonding device with the <computeroutput>bond0</computeroutput> interface name, as well as passes options to the bonding driver to configure it as an active-backup master device for the enslaved network interfaces. 
					</para>
				</listitem>
				<listitem>
					<para>
						 Edit the <filename>/etc/sysconfig/network-scripts/ifcfg-eth<replaceable>X</replaceable></filename> configuration file for both eth0 and eth1 so that the files show identical contents. For example: 
					</para>
					<screen><computeroutput>DEVICE=eth<replaceable>X</replaceable>USERCTL=no ONBOOT=yes MASTER=bond0 SLAVE=yes BOOTPROTO=none </computeroutput></screen><para>
						 This will enslave eth<replaceable>X</replaceable>(replace <replaceable>X</replaceable> with the assigned number of the Ethernet devices) to the bond0 master device. 
					</para>
				</listitem>
				<listitem>
					<para>
						 Create a network script for the bonding device (for example, <filename>/etc/sysconfig/network-scripts/ifcfg-bond0</filename>), which would appear like the following example: 
					</para>
					<screen><computeroutput>DEVICE=bond0 USERCTL=no ONBOOT=yes BROADCAST=192.168.1.255 NETWORK=192.168.1.0 NETMASK=255.255.255.0 GATEWAY=192.168.1.1 IPADDR=192.168.1.10 </computeroutput></screen>
				</listitem>
				<listitem>
					<para>
						 Reboot the system for the changes to take effect. 
					</para>
				</listitem>
			</orderedlist>
		</section>
		<section id="s2-hardware-pwrcfg">
			<title>Configuring a Fence Device</title>
			<indexterm>
				
				<primary>fence device</primary>
				<secondary>configuring</secondary>
			</indexterm>
			<para>
				 Fence devices enable a node to power-cycle another node before restarting its services as part of the failover process. The ability to remotely disable a node ensures data integrity is maintained under any failure condition. Deploying a cluster in a production environment <emphasis>requires</emphasis> the use of a fence device. Only development (test) environments should use a configuration without a fence device. Refer to <xref linkend="s2-hardware-pwrctrl" /> for a description of the various types of power switches. 
			</para>
			<para>
				 In a cluster configuration that uses fence devices such as power switches, each node is connected to a switch through either a serial port (for two-node clusters) or network connection (for multi-node clusters). When failover occurs, a node can use this connection to power-cycle another node before restarting its services. 
			</para>
			<para>
				 Fence devices protect against data corruption if an unresponsive (or hanging) node becomes responsive after its services have failed over, and issues I/O to a disk that is also receiving I/O from another node. In addition, if CMAN detects node failure, the failed node will be removed from the cluster. If a fence device is not used in the cluster, then a failed node may result in cluster services being run on more than one node, which can cause data corruption and possibly system crashes. 
			</para>
			<para>
				 A node may appear to <firstterm>hang</firstterm> for a few seconds if it is swapping or has a high system workload. For this reason, adequate time is allowed prior to concluding that a node has failed. 
			</para>
			<para>
				 If a node fails, and a fence device is used in the cluster, the fencing daemon power-cycles the hung node before restarting its services. This causes the hung node to reboot in a clean state and prevent it from issuing I/O and corrupting cluster service data. 
			</para>
			<para>
				 When used, fence devices must be set up according to the vendor instructions; however, some cluster-specific tasks may be required to use them in a cluster. Consult the manufacturer documentation on configuring the fence device. Note that the cluster-specific information provided in this manual supersedes the vendor information. 
			</para>
			<para>
				 When cabling a physical fence device such as a power switch, take special care to ensure that each cable is plugged into the appropriate port and configured correctly. This is crucial because there is no independent means for the software to verify correct cabling. Failure to cable correctly can lead to an incorrect node being power cycled, fenced off from shared storage via fabric-level fencing, or for a node to inappropriately conclude that it has successfully power cycled a failed node. 
			</para>
		</section>
		<section id="s2-hardware-upscfg">
			<title>Configuring UPS Systems</title>
			<indexterm>
				
				<primary>UPS systems</primary>
				<secondary>configuring</secondary>
			</indexterm>
			<para>
				 Uninterruptible power supplies (UPS) provide a highly-available source of power. Ideally, a redundant solution should be used that incorporates multiple UPS systems (one per server). For maximal fault-tolerance, it is possible to incorporate two UPS systems per server as well as APC Automatic Transfer Switches to manage the power and shutdown management of the server. Both solutions are solely dependent on the level of availability desired. 
			</para>
			<para>
				 It is not recommended to use a single UPS infrastructure as the sole source of power for the cluster. A UPS solution dedicated to the cluster is more flexible in terms of manageability and availability. 
			</para>
			<para>
				 A complete UPS system must be able to provide adequate voltage and current for a prolonged period of time. While there is no single UPS to fit every power requirement, a solution can be tailored to fit a particular configuration. 
			</para>
			<para>
				 If the cluster disk storage subsystem has two power supplies with separate power cords, set up two UPS systems, and connect one power switch (or one node's power cord if not using power switches) and one of the storage subsystem's power cords to each UPS system. A redundant UPS system configuration is shown in <xref linkend="fig-hardware-two-ups" />. 
			</para>
			<figure id="fig-hardware-two-ups">
				<title>Redundant UPS System Configuration</title>
				<mediaobject>
					<imageobject>
						<imagedata fileref="images/two_ups.png" />
					</imageobject>
					<textobject><para>
						 A redundant UPS system configuration. 
					</para>
					</textobject>
				</mediaobject>
			</figure>
			<para>
				 An alternative redundant power configuration is to connect the power switches (or the nodes' power cords) and the disk storage subsystem to the same UPS system. This is the most cost-effective configuration, and provides some protection against power failure. However, if a power outage occurs, the single UPS system becomes a possible single point of failure. In addition, one UPS system may not be able to provide enough power to all the attached devices for an adequate amount of time. A single UPS system configuration is shown in <xref linkend="fig-hardware-one-ups" />. 
			</para>
			<figure id="fig-hardware-one-ups">
				<title>Single UPS System Configuration</title>
				<mediaobject>
					<imageobject>
						<imagedata fileref="images/one_ups.png" />
					</imageobject>
					<textobject><para>
						 A single UPS system configuration. 
					</para>
					</textobject>
				</mediaobject>
			</figure>
			<para>
				 Many vendor-supplied UPS systems include &PROD; applications that monitor the operational status of the UPS system through a serial port connection. If the battery power is low, the monitoring software initiates a clean system shutdown. As this occurs, the cluster software is properly stopped, because it is controlled by a SysV runlevel script (for example, <filename>/etc/rc.d/init.d/rgmanager</filename>). 
			</para>
			<para>
				 Refer to the UPS documentation supplied by the vendor for detailed installation information. 
			</para>
			<section id="s3-hardware-partdisks">
				<title>Partitioning Disks</title>
				<indexterm>
					
					<primary>partitioning disks</primary>
				</indexterm>
				<para>
					 After shared disk storage has been set up, partition the disks so they can be used in the cluster. Then, create file systems or raw devices on the partitions. 
				</para>
				<indexterm>
					
					<primary><command>parted</command></primary>
					<secondary>creating disk partitions</secondary>
				</indexterm>
				<para>
					 Use <command>parted</command> to modify a disk partition table and divide the disk into partitions. While in <command>parted</command>, use the <command>p</command> to display the partition table and the <command>mkpart</command> command to create new partitions. The following example shows how to use <command>parted</command> to create a partition on disk: 
				</para>
				<itemizedlist>
					<listitem>
						<para>
							 Invoke <command>parted</command> from the shell using the command <command>parted</command> and specifying an available shared disk device. At the <prompt>(parted)</prompt> prompt, use the <command>p</command> to display the current partition table. The output should be similar to the following: 
						</para>
						<screen><computeroutput>Disk geometry for /dev/sda: 0.000-4340.294 megabytes Disk label type: msdos Minor Start End Type Filesystem Flags </computeroutput></screen>
					</listitem>
					<listitem>
						<para>
							 Decide on how large of a partition is required. Create a partition of this size using the <command>mkpart</command> command in <command>parted</command>. Although the <command>mkpart</command> does not create a file system, it normally requires a file system type at partition creation time. <command>parted</command> uses a range on the disk to determine partition size; the size is the space between the end and the beginning of the given range. The following example shows how to create two partitions of 20 MB each on an empty disk. 
						</para>
						<screen><computeroutput>(parted) 
						<userinput>mkpart primary ext3 0 20</userinput>(parted) 
						<userinput>mkpart primary ext3 20 40</userinput>(parted) 
						<userinput>p</userinput>Disk geometry for /dev/sda: 0.000-4340.294 megabytes Disk label type: msdos Minor Start End Type Filesystem Flags 1 0.030 21.342 primary 2 21.343 38.417 primary </computeroutput></screen>
					</listitem>
					<listitem>
						<para>
							 When more than four partitions are required on a single disk, it is necessary to create an <emphasis>extended partition</emphasis>. If an extended partition is required, the <command>mkpart</command> also performs this task. In this case, it is not necessary to specify a file system type. 
						</para>
						<note>
							<title>Note</title>
							<para>
								 Only one extended partition may be created, and the extended partition <emphasis>must</emphasis> be one of the four primary partitions. 
							</para>
						</note>
						<screen><computeroutput>(parted) 
						<userinput>mkpart extended 40 2000</userinput>(parted) 
						<userinput>p</userinput>Disk geometry for /dev/sda: 0.000-4340.294 megabytes Disk label type: msdos Minor Start End Type Filesystem Flags 1 0.030 21.342 primary 2 21.343 38.417 primary 3 38.417 2001.952 extended </computeroutput></screen>
					</listitem>
					<listitem>
						<para>
							 An extended partition allows the creation of <emphasis>logical partitions</emphasis> inside of it. The following example shows the division of the extended partition into two logical partitions. 
						</para>
						<screen><computeroutput>(parted) 
						<userinput>mkpart logical ext3 40 1000</userinput>(parted) 
						<userinput>p</userinput>Disk geometry for /dev/sda: 0.000-4340.294 megabytes Disk label type: msdos Minor Start End Type Filesystem Flags 1 0.030 21.342 primary 2 21.343 38.417 primary 3 38.417 2001.952 extended 5 38.447 998.841 logical (parted) 
						<userinput>mkpart logical ext3 1000 2000</userinput>(parted) 
						<userinput>p</userinput>Disk geometry for /dev/sda: 0.000-4340.294 megabytes Disk label type: msdos Minor Start End Type Filesystem Flags 1 0.030 21.342 primary 2 21.343 38.417 primary 3 38.417 2001.952 extended 5 38.447 998.841 logical 6 998.872 2001.952 logical </computeroutput></screen>
					</listitem>
					<listitem>
						<para>
							 A partition may be removed using <command>parted</command>'s <command>rm</command> command. For example: 
						</para>
						<screen><computeroutput>(parted) 
						<userinput>rm 1</userinput>(parted) 
						<userinput>p</userinput>Disk geometry for /dev/sda: 0.000-4340.294 megabytes Disk label type: msdos Minor Start End Type Filesystem Flags 2 21.343 38.417 primary 3 38.417 2001.952 extended 5 38.447 998.841 logical 6 998.872 2001.952 logical </computeroutput> </screen>
					</listitem>
					<listitem>
						<para>
							 After all required partitions have been created, exit <command>parted</command> using the <command>quit</command> command. If a partition was added, removed, or changed while both nodes are powered on and connected to the shared storage, reboot the other node for it to recognize the modifications. After partitioning a disk, format the partition for use in the cluster. For example, create the file systems for shared partitions. Refer to <xref linkend="s3-hardware-createfs" /> for more information on configuring file systems. 
						</para>
						<para>
							 For basic information on partitioning hard disks at installation time, refer to the <citetitle>&RHELIG;</citetitle>. 
						</para>
					</listitem>
				</itemizedlist>
			</section>
			<section id="s3-hardware-createfs">
				<title>Creating File Systems</title>
				<indexterm>
					
					<primary>file systems</primary>
					<secondary>creating</secondary>
				</indexterm>
				<indexterm>
					
					<primary><command>mkfs</command></primary>
				</indexterm>
				<indexterm>
					
					<primary>ext3</primary>
				</indexterm>
				<para>
					 Use the <command>mkfs</command> command to create an ext3 file system. For example: 
				</para>
				<screen><command>mke2fs -j -b 4096 /dev/sde3</command></screen><para>
					 For optimal performance of shared file systems, make sure to specify a 4 KB block size with the <command>mke2fs -b</command> command. A smaller block size can cause long <command>fsck</command> times. 
				</para>
			</section>
		</section>
	</section>
</chapter>

